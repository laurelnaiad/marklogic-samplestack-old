{"Body":"<p>There are annotation tasks where the items belong to multiple categories and annotators have to mark each category to which the item belongs.</p>\n\n<p>e.g: the same coder c1 assigns the two categories (v1,v2) to the item '1'</p>\n\n<blockquote>\n  <p>task = AnnotationTask(data=[(‘c1’, ‘1’, ‘v1’),(‘c1’, ‘1’, ‘v2’),...])</p>\n</blockquote>\n\n<p>So should such multiple categories be represented as bitstrings , such that for n categories there would be a whopping 2^n assignments ? This would surely make the inter annotator agreement (IAA) scores very low for minor differences.</p>\n\n<p>I would like to capture diverse partial agreement and assign high weightage to specific categories reflecting their importance to be annotated correctly. Moreover I would like the metric to significantly reflect even minor agreement.</p>\n\n<p>Considering the above, what is the best way to compute annotation agreement for tasks that require multiple assignment to an item? And how to represent categories for such cases?</p>\n", "Id":"4248", "LastEditorUserId":"1758", "Title":"Annotation agreement for multiple category assignment", "CreationDate":"2013-08-25T08:19:33.420", "OwnerUserId":"1758", "PostTypeId":"1", "Tags":"<corpora><annotation>", "AnswerCount":"1", "comments":[], "LastEditDate":"2013-08-25T15:05:12.813", "LastEditedUser":{"UpVotes":"5", "DownVotes":"1", "ProfileImageUrl":"http://i.stack.imgur.com/EVrHW.jpg", "Id":"1758", "AccountId":"904615", "Views":"4", "Reputation":"170", "CreationDate":"2013-02-13T05:16:10.560", "DisplayName":"Ali", "LastAccessDate":"2013-10-15T10:46:17.913"}, "ViewCount":"38", "LastActivityDate":"2013-08-25T15:05:12.813", "Score":"2", "OwnerUser":{"UpVotes":"5", "DownVotes":"1", "ProfileImageUrl":"http://i.stack.imgur.com/EVrHW.jpg", "Id":"1758", "AccountId":"904615", "Views":"4", "Reputation":"170", "CreationDate":"2013-02-13T05:16:10.560", "DisplayName":"Ali", "LastAccessDate":"2013-10-15T10:46:17.913"}}