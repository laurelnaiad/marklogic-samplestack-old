{"OwnerUser":{"UpVotes":"125", "WebsiteUrl":"http://www.cs.mcgill.ca/~akazna/", "Id":"33", "AccountId":"459207", "CreationDate":"2011-09-13T20:58:42.087", "AboutMe":"<p>From the School of Computer Science and Department of Psychology at McGill University, I marvel at the world through algorithmic lenses. My specific interests are in quantum computing, evolutionary game theory, modern evolutionary synthesis, and theoretical cognitive science. Previously I was at the Institute for Quantum Computing and Department of Combinatorics &amp; Optimization at the University of Waterloo and a visitor to the Centre for Quantum Technologies at the National University of Singapore.</p>\n\n<ul>\n<li><a href=\"http://egtheory.wordpress.com/\" rel=\"nofollow\">Theory, Evolution, and Games Group Blog</a></li>\n<li><a href=\"http://cstheory.blogoverflow.com/author/artemkaznatcheev/\">Theoretical Computer Science Blogoveflow</a></li>\n<li><a href=\"https://plus.google.com/u/0/101780559173703781847/\" rel=\"nofollow\">G+ profile</a></li>\n</ul>\n", "Age":"25", "Location":"Montreal, Canada", "DownVotes":"18", "Views":"25", "Reputation":"876", "DisplayName":"Artem Kaznatcheev", "LastAccessDate":"2014-01-11T00:59:33.567"}, "comments":[], "Body":"<p>I was hoping @Mitch would expand on his comment about PAC-learning. For now, I will provide the only application of PAC-learning directly to linguistics that I know. It is in Ronald de Wolf's master's thesis: <a href=\"http://homepages.cwi.nl/~rdewolf/publ/philosophy/phthesis.pdf\" rel=\"nofollow\">Philosophical Applications of\nComputational Learning Theory</a>.</p>\n\n<p>de Wolf argues for the Chomskian stance on poverty of the stimulus in much the same way as Gold. Except, instead of showing that the context-free languages are not learnable in the limit from positive instances (as Gold), he instead shows that <strong>context-free languages are not effeciently <a href=\"http://en.wikipedia.org/wiki/Probably_approximately_correct_learning\" rel=\"nofollow\">PAC-learnable</a></strong> from positive and negative instances. </p>\n\n<p>Note that PAC-learning is much more reasonable model than Gold's since it allows errors of two kinds: (1) some very small fraction of children completely failing to learn the language, and (2) even the successful children making occasional (again small fraction of instances) mistakes on specific sentences, but getting the overwhelming majority of the language correct. These seem reasonable.</p>\n\n<p>The two restrictions placed on the learners (different from Gold) are: (1) that the algorithm must be polynomial time (i.e. efficient), and (2) the analysis is over worst-case distributions over inputs. The first restriction is perfectly reasonable if we are of the brain as computers camp. The second restriction is unreasonable (children don't get arbitrary distributions, they get very specific ones sometimes involving things like motherese), but still more reasonable than Gold's restriction of a completely adversarial presentation of sentences (that just lists them only in the limit).</p>\n\n<p>Unfortunately, I don't think de Wolf's thesis was unnoticed by lingusitics, since <a href=\"http://scholar.google.ca/scholar?cites=6476994582132477475&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en\" rel=\"nofollow\">the only citations to it</a> is from a recent non-linguistics paper by another theoretical computer scientist.</p>\n", "Id":"2023", "ParentId":"1066", "CreationDate":"2012-05-31T02:29:31.687", "Score":"2", "PostTypeId":"2", "OwnerUserId":"33", "LastActivityDate":"2012-05-31T02:29:31.687", "CommentCount":"2"}