{"comments":[], "Body":"<p><em>Preliminary note: I tried to be as general as possible, but the variety of\nsyntactic descriptions of Natural Language is such that it is unlikely\nthat everyone will find here his preferred brand of syntax or parsing. For example, the parsing of lexicalized descriptions of language is not directly accounted for. I am not sure that this\ncan be helped in any reasonnable way. My purpose here has been mainly to explore the range of questions that could or should be addressed when considering the complexity of natural language parsing.</em></p>\n\n<p>The first point is that \"<em>Parsing of a human-language text</em>\" is an ill defined task. Complexity analysis is a mathematical endeavor which makes sense only within a mathematically well defined framework. You can therefore ask such a question only with respect to a given formal description of the language syntax.</p>\n\n<p>Another preliminary point is that parsing is supposed to analyze syntax only, not semantics. But where is the border between syntax and semantics.</p>\n\n<p><strong>What is parsing ?</strong></p>\n\n<p>Parsing is always done with respect to a grammar (term used loosely to\nmean a description of the syntax) of a language in some appropriate\nformalism (formal framework). So the complexity classes are also\nclassifying these formalisms.</p>\n\n<p>Secondly, the concept of parsing itself should be precisely defined,\nIt is clear that it is an algorithm taking as input a sentence\n(meaning only a linear sequence of words) and produces as output a\nstructure that give a more explicit description of the sentence\nemphisizing the structural properties that make it belong to the language specified by the given\ngrammar (I am trying very hard not to be too specific).</p>\n\n<p>For example, with a generative grammar, the output of the parsing\nprocess may be a structure that describes precisely how the sentence\ncan be generated by that grammar, when it is syntactically correct,\ni.e. when it belongs to the language described.</p>\n\n<p>Furthermore, it is known that natural languages are generally\nsyntactically ambiguous, i.e., that some syntactically correct\nsentences can be structurally described in several ways (generated in\ndifferents ways). Note that a sentence can be syntactically ambiguous,\neven though there is no semantic ambuiguity.  Now the result expected\nfrom parsing could be :</p>\n\n<ul>\n<li>just be a yes/no answer stating whether the sentence is syntactically\ncorrect. This is usually called <em>recognition</em>, rather than parsing;</li>\n<li>any of the possible structural descriptions of the input sentence;\nwhen it is syntactically correct.</li>\n<li>all the structural descriptions of the input sentence; when it is\nsyntactically correct.  This itself opens a spectrum of possibilities\nas there may be different ways of representing this (possibly\ninfinite) set of parses, which may be more space efficient or easier\nto use for later stages of the sentence analysis.  See for example\nthe question <a href=\"http://linguistics.stackexchange.com/questions/4619/\">Is there a favoured data structure for storing ambiguous parse trees in Natural Language Processing?</a> ;</li>\n<li>one or more structural descriptions of the input sentence, when it is\nsyntactically correct, satisfying some preference or selection\ncriterion.</li>\n</ul>\n\n<p>In addition, the nature of the expected structural description may\nvary. Typically, the result of parsing a sentence according to a <a href=\"http://en.wikipedia.org/wiki/Tree-adjoining_grammar\" rel=\"nofollow\">tree\nadjoining grammar</a> (TAG) can be a (set of) derivation(s) tree(s) or a\n(set of) derived tree(s). Derivation trees and derived trees are\ndistinct for TAG.</p>\n\n<p>It is not obvious that the complexity is the same for all choices of\nwhat is expected as parsing result for a given formalism.</p>\n\n<p><strong>What is complexity ?</strong></p>\n\n<p>If the formalisms, and the result expected from the parsing process,\nare properly defined, it is of course possible to classify the parsing\nprocess for that formalism of in some complexity class, or more\nprecisely to do a complexity analysis of the parsing process, as\ncomplexity classes come in many flavors : time complexity or space\ncomplexity, asymptotic worst case complexity or average complexity.</p>\n\n<p>Though people often think of asymptotic worst case time complexity, it\nmay make more sense in practice to consider average complexity. So\nsuch clssifications into complexity classes should be interpreted with\ncare from a pragmatic point of view.</p>\n\n<p>Another point is that complexity is often considered with respect to\nthe size (number of words) of the sentence to be parsed. But\ncomplexity may also be analyzed with respect to the size of the\ngrammar used to analyze a sentence. This is important as some people\ntend to increase the size of the syntax description in order to\ncapture rarer or more subtle phenomena. This does not come for free.</p>\n\n<p>Many formalisms are composed of a formal generative skeleton (such as\ncontext-free grammars) where various attributes (features,\nprobabilities, ...) can be associated to the rules constituents,\nhaving to respect various constraints associated to the rule. The\naddition of these attributes and the respect of these constraints\nincreases, often drastically, the complexity of the formalism, making\nit NP-hard, and even possibly Turing-complete (allowing you to encode\nas a parsing problem any algorithm you can dream of).</p>\n\n<p>Of course there are zillions of subcases providing interesting\ncomplexity problems to people inclined to such studies.</p>\n\n<p>One aspect commonly analyzed is the complexity of skeleton formalisms,\nwithout any attribute. Typically parsing context-free languages has\ntime complexity O(n³), which is fairly low and quite tractable in\npractice. However, some structural organizations of the sentences\ncannot be captured by CF grammar. A classical example is <a href=\"http://www.eecs.harvard.edu/~shieber/Biblio/Papers/shieber85.pdf\" rel=\"nofollow\">cross-serial\ndependency</a>.</p>\n\n<p>Rather than add attributes to somehow encode the information necessary\nto handle such structures in a very general feature mechanism, some\nscientists consider that it is more effective, and possibly more\nperspicuous, to complexify a bit the skeleton formalism. This for\nexample lead to the development of Tree Adjoining Grammars (time\ncomplexity O(n⁶)), then to a hierarchy of formalisms with increasing\npolynomial complexity, and the so-called <a href=\"http://www.kornai.com/MatLing/mcsfin.pdf\" rel=\"nofollow\">mildly context-sensitive\nformalisms</a> and  <a href=\"http://acl.ldc.upenn.edu/P/P92/P92-1018.pdf\" rel=\"nofollow\">Linear Context-Free rewriting\nsystems</a>.  One remarkable aspect is that, though the polynomial\nworst-case complexity may seem high, structures producing high\ncomplexity may be few, so that things remain very tractable in\npractice.</p>\n\n<p>This is to underscore, again, that  complexity analysis\ndoes not necessarily tell the whole story.</p>\n", "Id":"6129", "ParentId":"3629", "CreationDate":"2013-12-13T15:32:11.860", "Score":"0", "PostTypeId":"2", "OwnerUserId":"2129", "LastActivityDate":"2013-12-13T15:32:11.860", "OwnerUser":{"UpVotes":"15", "DownVotes":"0", "ProfileImageUrl":"http://i.stack.imgur.com/zegup.png", "Id":"2129", "AccountId":"2350982", "Views":"3", "Reputation":"246", "CreationDate":"2013-06-03T21:52:14.630", "DisplayName":"babou", "LastAccessDate":"2014-01-11T10:17:24.477", "AboutMe":"<p>The name Babou was given to me by my grandson (two years old at the time). It actually means grandpa in Armenian and Swahili.</p>\n\n<p>.</p>\n\n<p>.</p>\n\n<p>.</p>\n\n<p>.</p>\n\n<p>babou AT inbox.com</p>\n"}}