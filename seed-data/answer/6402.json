{"OwnerUser":{"UpVotes":"432", "WebsiteUrl":"", "Location":"United States", "Id":"4", "DownVotes":"2", "AccountId":"167219", "Views":"42", "Reputation":"1906", "CreationDate":"2011-09-13T20:24:01.087", "DisplayName":"Mitch", "LastAccessDate":"2014-01-15T11:56:51.610", "AboutMe":"<p>Native speaker of American English (AmE) with touches of Southern AmE.\nProficient in French and German. Elementary knowledge of Mandarin. Basic knowledge of linguistics.</p>\n"}, "comments":[], "Body":"<p>As with all natural laws, Zipf's law is an approximation. If you take a large corpus, and compute the Zipf curve, it will more or less follow a Zipf distribution (with coefficients  thrown in to account for slack.</p>\n\n<p>This doesn't mean that for every language it follows the exact rule of 'the second most common lexical item is 1/2 as frequent as the most common'. It's just a lax observation. One can do a regression analysis to discover exactly the coefficients for a particular language.</p>\n\n<p>Even within a language there are divergences. It isn't hard to find works whose Zipf curves diverge from the general language's. Joyce's Finnegan's Wake uses so many rare and made-up words that its tail is thick and long. But children's literature attempts to be easily understood and so have few rare words and drops off much more sharply.</p>\n\n<p>Zipf's law doesn't just approximate word frequencies but also letter frequencies, city sizes, income ranks, and many other rank vs. frequency graphs. It is taken into account in decrypting substitution ciphers, and also in creating codes (artificial languages) that do not follow Zipf's law.</p>\n", "Id":"6402", "ParentId":"6371", "CreationDate":"2014-01-14T19:49:21.513", "Score":"0", "PostTypeId":"2", "OwnerUserId":"4", "LastActivityDate":"2014-01-14T19:49:21.513", "CommentCount":"1"}