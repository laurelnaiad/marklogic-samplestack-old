{"OwnerUser":{"UpVotes":"5", "WebsiteUrl":"", "DownVotes":"1", "Id":"657", "AccountId":"1160318", "Views":"3", "Reputation":"526", "CreationDate":"2012-01-10T02:11:47.230", "DisplayName":"Jeremy Needle", "LastAccessDate":"2014-01-18T15:40:44.497", "AboutMe":"<p>Sociolinguist.</p>\n"}, "comments":[], "Body":"<p>This is an interesting question, but there are a number of decisions you need to make about terms.</p>\n\n<p>Are you talking about phonemes or graphemes ('letters')? If it's graphemes, you'd need to perform some conversion between the two (a fairly messy but doable process, because English spelling is non-transparent), because 'phonological rules' are going to be over phonemes.</p>\n\n<p>What kind of measure of 'conformity' are you thinking about? There are a lot of issues here, both theoretical and practical. It's not a settled question as to what English speakers use to determine 'word goodness', but there are many hypotheses involving both frequency- and neighbor-based approaches; even ignoring the human side, you have to determine a useful measure. A plain legal/illegal distinction is much simpler to obtain, but it sounds like you need more.</p>\n\n<p>In what way do you operationalize 'English phonological rules'? Computationally, n-gram methods are common, but phonologists have also made use of various rule-based frameworks.</p>\n", "Id":"5878", "ParentId":"5877", "CreationDate":"2013-11-10T17:17:16.600", "Score":"1", "PostTypeId":"2", "OwnerUserId":"657", "LastActivityDate":"2013-11-10T17:17:16.600", "CommentCount":"1"}