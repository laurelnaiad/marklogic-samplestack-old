{"Body":"<p>FLN is a retrenchement of a much stronger claim, which is the claim of universal grammar. Since you asked about Universal Grammar, not just FLN, I will try to answer the UG question.</p>\n\n<p>The claim of universal grammar is that any sentence, no matter how complex or embedded, can be translated between any two languages <em>with essentially the same kind of embedding structure</em>. This would mean that aside from superficial differences in vocabulary and word order, that all languages share a deeply isomorphic stack-parsed tree grammar at their core.</p>\n\n<p>To show you that this makes nontrivial predictions, here is a super-embedded English sentence:</p>\n\n<ul>\n<li>I cut the tree which held the girl which put the frog which swallowed the fly in its throat in her pocket down quickly.</li>\n</ul>\n\n<p>I will translate this to Hebrew:</p>\n\n<ul>\n<li>Kara'teti et ha-etz she-hichzik et ha-yalda she-sama et ha-tsfardea sh-bala' et ha-zvuv ba-garon shelo ba-kis she-la be-mehirut.</li>\n</ul>\n\n<p>This translation is word for word, with the exact same recursive structure, except the final \"down\" in English is omitted (since cut-down is a peculiar English structure). The embedding clause structure and the general sentence pattern is exactly the same, even though the two languages are not particularly related. The recursion works exactly the same.</p>\n\n<p>Hebrew is a revived language, but you can do this with Chinese, with French, with any old-world language, basically, and you will produce a sentence with essentially the same nesting structure, with perhaps some word order or clause-order differences, but which can be recognized to be basically the same sentence, because the parse-tree structure is largely isomorphic, with the same levels of nesting.</p>\n\n<p>That's the key prediction of UG.</p>\n\n<h3>Made up constructions that violate UG</h3>\n\n<p>The statement that all languages recurse in this way is to be contrasted with made-up recursive structures which <em>never ever</em> occur in natural languages:</p>\n\n<p>Here are sentences in a made-up language, with the same vocabulary as English, but where the following sentence is grammatical:</p>\n\n<p>\"I walked to the store is green.\"</p>\n\n<p>The intended semantics of this construction are</p>\n\n<p>(I walked to the store) and (the store is green).</p>\n\n<p>Since \"the store\" repeats in both sentences, one could imagine a culture where you could say:</p>\n\n<p>\"I walked to the store is green is my favorite color looks better than Nancy's favorite color is yellow.\"</p>\n\n<p>Intended Meaning \"I walked to the store\"/\"the store is green\"/\"green is my favorite color\"/\"my favorite color looks better than Nancy's favorite color\"/\"Nancy's favorite color is yellow.\"</p>\n\n<p>In english, you would say it as follows:</p>\n\n<ul>\n<li>\"I walked to the store <em>which</em> is green <em>which</em> is my favorite color <em>which</em> looks better than Nancy's favorite color <em>which</em> is yellow.\"</li>\n</ul>\n\n<p>Those \"which\"es are necessary, and something like it is necessary in <em>all</em> the world's languages. The reason is that you are not allowed to make overlapping constructions</p>\n\n<ul>\n<li>(I walked to the store) is green</li>\n<li>I walked to (the store is green)</li>\n</ul>\n\n<p>you can see that without the \"which\", the parentheses overlap. When parentheses overlap, the language is not parsable by a stack automaton, meaning that it isn't the type of context-free grammar that Chomsky identified as central to human language grammatical embedding.</p>\n\n<p>The \"which\" makes the stuff that follows subordinate to the preceding stuff, so you get</p>\n\n<p>(I walked to (the store which is (green which is (my favorite color which is better than (Nancy's favorite color which is yellow)))))</p>\n\n<p>notice that the units don't overlap, so this is ok. That's a major nontrivial prediction of UG: all language embedding must be non-overlapping.</p>\n\n<h3>Exceptions</h3>\n\n<p>when languages have complex case systems, they sometimes have a freer word order, which allows units which are grouped together to slide apart because no ambiguity is possible, because of the case markings. Such things might lead structures to partially overlap, which contradicts UG. This is not so terrible, because you can usually figure out how to translate any such thing back and forth with the same \"deep structure\", essentially the same parse tree description.</p>\n\n<p>The more serious challenge is from Piraha and Warlpiri, which do not allow complex embedding at all. You couldn't even translate the Boy-frog-tree sentence to Piraha, you would need many separate sentences. These are counterexamples to the claims of UG.</p>\n\n<h3>Why stacks?</h3>\n\n<p>So UG is a predictive, powerful, statement. It just happens to be wrong, despite the fact that you have to go to the far corners of the Earth for a true counterexample. That's weird. Why should it be true of <em>almost</em> all languages, with a few isolated exceptions?</p>\n\n<p>One hypothesis you could make is that bilingual speakers and the need for translation were able to homogenize the grammar between neighboring languages over time, leading the world's grammars to standardize on essentially the same tree recursive form.</p>\n\n<p>But from my experience with mathematical formalisms, and with artificial languages, I have noticed that human beings are just \"hard wired\" to parse stack languages naturally. This is most notable in computer languages, where PASCAL (which is rigidly structured and very heirarchical) is considered super-elegant, LISP (which is essentially all nested parentheses) is considered the king of elegance, while \"C\" (which has side effects) is considered ugly.</p>\n\n<p>Despite its ugliness C wins.</p>\n\n<p>The same is true in mathematics, where index notation for tensors is considered \"ugly\", and function notation is preferred, with nested parentheses. This despite the <em>obvious</em> fact that index notation is a language that perfectly fits the domain, and parentheses notation does not. The same problems occur in Fregian semantics, in biochemical modeling langauges, in Aristotelian philosophy (where categories are not allowed to overlap), in object oriented languages (no multiple inheritence), and basically everywhere the humans try to design rules for natural things: they almost always make nonoverlapping structures, even when these are wrong for the intended domain.</p>\n\n<p>I speculate that the human brain is just hard-wired for tree grammars because of some biochemistry. If memory is encoded in specific RNA chains in the brain, as I am sure it is, it is possible that sliding certain RNA chains in one direction makes a \"push\" operation on a stack, while sliding them in the other direction makes a pop. Then the one-dimensional linear stack structure of language would reflect the biochemistry of the memory storing RNA in the brain, as the ears respond to the input. The existence of a stack in the brain, an actual biochemical stack, would explain this peculiarity. But this is just speculation at this point.</p>\n", "Id":"1564", "ParentId":"1533", "LastEditorUserId":"810", "CreationDate":"2012-03-04T07:45:06.130", "OwnerUserId":"810", "PostTypeId":"2", "comments":[], "LastEditDate":"2012-03-04T15:03:21.233", "LastEditedUser":{"UpVotes":"39", "WebsiteUrl":"", "Id":"810", "AccountId":"528458", "CreationDate":"2012-03-03T03:39:07.707", "AboutMe":"<p>Everett is right. Chomsky was wrong. I want to know how recursion spread from Greece through the old world, and it seems we will find out. I am also curious about simpler examples of recursion, like list making.</p>\n\n<p>I mostly blindly follow Gell-Mann and agree with Joseph Greenberg about superfamilies and mass-comparison. I don't see any reason that the mass-comparison method cannot be made rigorous with good statistics.</p>\n\n<p>I like formal grammars, and I don't believe I understand something unless I can program a computer to understand it the same as me. My linguistic goal today is to write a good English parser. I think I can do at least marginally better than current parsers, which seem to be weighed down with theoretical cruft. Maybe it'll read a newspaper, maybe not. I hope to find out soon enough.</p>\n", "Age":"41", "Location":"New York City", "DownVotes":"0", "Views":"92", "Reputation":"481", "DisplayName":"Ron Maimon", "LastAccessDate":"2014-01-02T00:11:46.293"}, "LastActivityDate":"2012-03-04T15:03:21.233", "Score":"1", "OwnerUser":{"UpVotes":"39", "WebsiteUrl":"", "Id":"810", "AccountId":"528458", "CreationDate":"2012-03-03T03:39:07.707", "AboutMe":"<p>Everett is right. Chomsky was wrong. I want to know how recursion spread from Greece through the old world, and it seems we will find out. I am also curious about simpler examples of recursion, like list making.</p>\n\n<p>I mostly blindly follow Gell-Mann and agree with Joseph Greenberg about superfamilies and mass-comparison. I don't see any reason that the mass-comparison method cannot be made rigorous with good statistics.</p>\n\n<p>I like formal grammars, and I don't believe I understand something unless I can program a computer to understand it the same as me. My linguistic goal today is to write a good English parser. I think I can do at least marginally better than current parsers, which seem to be weighed down with theoretical cruft. Maybe it'll read a newspaper, maybe not. I hope to find out soon enough.</p>\n", "Age":"41", "Location":"New York City", "DownVotes":"0", "Views":"92", "Reputation":"481", "DisplayName":"Ron Maimon", "LastAccessDate":"2014-01-02T00:11:46.293"}}