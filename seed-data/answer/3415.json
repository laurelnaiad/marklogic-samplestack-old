{"OwnerUser":{"UpVotes":"102", "WebsiteUrl":"", "Location":"Seoul, South Korea", "Id":"1055", "DownVotes":"9", "AccountId":"1453460", "Views":"21", "Reputation":"1947", "CreationDate":"2012-05-29T05:50:57.073", "DisplayName":"acattle", "LastAccessDate":"2014-01-17T00:59:41.983", "AboutMe":"<p>I am currently studying for an MA in Computational Linguistics even though my background is not in linguistics. I discovered Computational Linguistics while receiving my B.A.Sc in Software Engineering when I built a basic bigram parser for an AI assignment. This caused me to become interested in linguistics and to take a few introductory level courses. I currently have completed my first year of my masters and am currently looking for a thesis topic.</p>\n"}, "comments":[], "Body":"<p><em>Note: I am a little confused on exactly what the OP means by \"similar\" and if they want to find English-like strings or actual English strings. I may need to edit my response once they provide clarification</em></p>\n\n<p>This is the type of feature where the question isn't \"how accurate can I make it?\" but rather \"how good is good enough?\" as some of the methods will be easier to implement and more efficient to run than others. Also, there's a very big difference between identifying <em>English-like words</em> and identifying <em>English words</em> so I'm going to break my answer into two parts.\n<br>\n<br></p>\n\n<h3>Finding English-like Text</h3>\n\n<p>For this purpose, your bigram/trigram probability method is probably satisfactory. If you haven't researched it already, there are plenty of online resources for teaching people how to compute probabilities from bigrams. <a href=\"http://www.cs.columbia.edu/~kathy/NLP/ClassSlides/Class3-ngrams09/ngrams.pdf\" rel=\"nofollow\">Here's a set of lecture notes I randomly found on Google</a>. If you find you're getting a lot of strings with a probability of 0 then you should look into using some kind of <a href=\"http://en.wikipedia.org/wiki/N-gram#Smoothing_techniques\" rel=\"nofollow\">n-gram smoothing technique</a> but this is probably less important with letter-based n-grams than with word-based n-grams.</p>\n\n<p>However, I see one major problem with using off-the-shelf probabilities, like the <a href=\"http://norvig.com/mayzner.html\" rel=\"nofollow\">Mayzner frequency list</a> you're currently using: <strong>n-grams do not cross word boundaries</strong>. You correctly say that bigrams like \"WZ\" never occurs <em>word-internally</em> in English, thus any \"WZ\" bigram would be assigned a probability of 0 (and thus the probability of the entire string by <a href=\"http://en.wikipedia.org/wiki/Naive_Bayes_classifier\" rel=\"nofollow\">Naive Bayes</a>). Problem is that there's nothing stopping them from occurring <strong>across word boundaries</strong>. For example, the sentence \"I don't care how Zoe feels\" is obviously English but if you remove spaces, punctuation, and capitalize everything you get \"IDONTCAREHOWZOEFEELS\" which <strong>does</strong> contain the bigram \"WZ\".</p>\n\n<p>One possible solution would be to calculate your own probabilities by taking a large corpus of text, like the <a href=\"http://en.wikipedia.org/wiki/Brown_Corpus\" rel=\"nofollow\">Brown Corpus</a>, processing the text so that it is a continuous string of capital letters, and then using that to calculate your own probabilities. See <a href=\"http://www.ee.columbia.edu/~stanchen/e6884/labs/lab3/x43.html\" rel=\"nofollow\">here</a> if you need help implementing this.</p>\n\n<p>You also mention about using consonant/vowel ratios and individual letter frequencies to improve your result. You may not realize it but what you're describing is <a href=\"http://en.wikipedia.org/wiki/Katz%27s_back-off_model\" rel=\"nofollow\">Katz's back-off model</a> which uses trigram, bigram, <strong>and</strong> unigram probabilities to evaluate text.\n<br>\n<br></p>\n\n<h3>Finding Actual English Text</h3>\n\n<p>You used the word \"rank\" in your question so I think these methods may not be what you want since they will tend to return more of a \"yes/no\" answer than a probability. That isn't to say you can't adapt them to give a probability but it may be difficult.</p>\n\n<p>The first question is do you want to find which strings <em>contain</em> and English word or which string are <em>entirely English</em>. To illustrate the difference, think about which of these two you'd rank higher:</p>\n\n<blockquote>\n  <p>ORANGES<br>\n  QAPPLEQ</p>\n</blockquote>\n\n<p>Finding strings which contain an English word is conceptually very easy but requires a lot of time and computing power. You'd simply search for ever word in an English dictionary in every string. One major problem is that there are <em>a lot</em> of English words. If you narrow your search down to the top 3000 or so most common English words then your speed increases but your accuracy drastically decreases. Another problem is that there are single-letter words in English (\"a\" and \"I\") that, unless excluded from the word list, would mean the vast majority of strings contain an English word.</p>\n\n<p>If you want to find which strings actually <em>are English</em> then you should focus on <strong>identifying word boundaries in text</strong> (as opposed to in spoken language). The good news is that there has already been research on on this problem. The bad news is that the existing research generally doesn't focus on English. I have found articles explicitly about identifying word boundaries in <a href=\"http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/paisarn/papers/apccas98.pdf\" rel=\"nofollow\">Thai text</a> and in <a href=\"http://rws.xoba.com/newindex/cpcol.pdf\" rel=\"nofollow\">Chinese Text</a>. These languages generally do not use white spaces so finding methods to break text into individual words is very important for any kind of computational linguistic analysis. Unfortunately, both these papers seem to use statistical methods so I'm not sure how applicable they are to English text (since the required statistics may not be readily available). You may also want to read <a href=\"http://stackoverflow.com/questions/1605353/word-break-in-languages-without-spaces-between-words-e-g-asian\">this Stackoverflow question</a> about identifying word boundaries in Japanese, but again you have a hard time applying this to your situation.</p>\n\n<p>I have found <a href=\"http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT6185524&amp;id=F-UGAAAAEBAJ&amp;oi=fnd&amp;dq=word+boundary+identification&amp;printsec=abstract#v=onepage&amp;q=word%20boundary%20identification&amp;f=false\" rel=\"nofollow\">this patent</a> for identifying word boundaries in continuous text (which is exactly what you'd need) <strong>but</strong> being a patent, you may need to license this technology before you can be allowed to use it (even for non-commercial uses).</p>\n", "Id":"3415", "ParentId":"3414", "CreationDate":"2013-03-22T02:42:00.087", "Score":"3", "PostTypeId":"2", "OwnerUserId":"1055", "LastActivityDate":"2013-03-22T02:42:00.087", "CommentCount":"7"}