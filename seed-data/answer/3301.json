{"Body":"<p>At the phoneme level, a text-to-speech system is usually tied to the phoneme set of the language a voice was built from. This is usually because modern speech synthesizers (MBROLA, Cepstral, AT&amp;T, etc.) use recorded voice samples in a diphone database (sampling phoneme pairs from the recorded data). This allows them to sound more natural.</p>\n\n<p>As a result of this, the phoneme set is restricted to the voice's language. Thus, handling IPA phonemes not in the target voice is one problem you need to solve in interpreting IPA phoneme strings.</p>\n\n<p>This is further complicated as the underlying phoneme model is likely a simplistic one that does not model phoneme features (e.g. voiceless bilabial fricative) that can be mapped from one phoneme scheme to another.</p>\n\n<p>The eSpeak text-to-speech engine comes close to this in that its model is based around phoneme features and it can support a large number of languages, but it has taken the choice of using a language per voice.</p>\n\n<p>I am in the process of building the Cainteoir text-to-speech engine that will use phoneme features to model phonemes, providing transcription schemes to map to/from that model and separating language and voice. That is, each language and voice specify the phonemes they support, allowing a voice to speak different languages and each language to be used by multiple voices.</p>\n\n<p>The other problem comes in identifying what text is IPA and what is not. This is complex as /jes/ could be IPA or could be jes in italics. Also, gin could be IPA or could be a word depending on context. This can be seen in the way different text-to-speech systems handle roman numerals (I, II, III, IV, V, ...): consider \"Chapter I was the Chapter I was reading.\" and \"Henry IV was placed on an IV drip.\"</p>\n\n<p>Text-to-speech programs will often support SSML (Speech Synthesis Markup Language) that allows you to write &lt;phoneme alphabet=\"ipa\" ph=\"jes\">yes&lt;/phoneme> to specify IPA phonemes. Different text-to-speech programs will handle this differently, for example Cepstral only supports their Arpabet-like phonemes.</p>\n\n<p>UPDATE:</p>\n\n<p>The text-to-speech engine will express phonemes in their own phoneme transcription scheme that expresses the phonemes the voice supports. For Cepstral this is based on Arpabet (the US voice uses the CMU pronunciation dictionary with lower case phonemes); for MBROLA this is based on SAMPA (with each voice supporting the corresponding language-based SAMPA phonemes and some additional phonemes); for eSpeak this is based on Kirshenbaum.</p>\n\n<p>The problem then becomes (a) reading the IPA text and (b) mapping the phonemes to the phonemes supported by the voice. A lot of text-to-speech engines don't bother as they do not have the need to support this (dictionary entries are specified in the voice's phoneme transcription scheme).</p>\n\n<p>The approach I am taking is to express a phoneme transcription scheme as a collection of <code>text =&gt; features</code> mappings such as <code>/s/ {vls,alv,frc}</code> (ipa: <a href=\"https://raw.github.com/rhdunn/cainteoir-engine/master/data/phonemes/ipa.phon\" rel=\"nofollow\">https://raw.github.com/rhdunn/cainteoir-engine/master/data/phonemes/ipa.phon</a>, ascii-ipa: <a href=\"https://raw.github.com/rhdunn/cainteoir-engine/master/data/phonemes/ascii-ipa.phon\" rel=\"nofollow\">https://raw.github.com/rhdunn/cainteoir-engine/master/data/phonemes/ascii-ipa.phon</a>).</p>\n\n<p>The idea here is to store the phonemes internally as a sequence of feature groups, allowing <code>transcription =&gt; features =&gt; transcription</code>. This then supports, for example, MBROLA voices reading Unicode-based IPA transcriptions. Here, each voice will have their own <code>features =&gt; transcription</code> mapping that maps several feature groups to the same transcription. For example, an English voice could pronounce /r/ and /ɹ/ as /ɹ/ as <code>{vcd,alv,trl}</code> and <code>{vcd,alv,apr}</code> could map to the same phoneme /r/.</p>\n\n<p>UPDATE 2:</p>\n\n<p>Using <strong>ˌoʊkeɪˈhiːɹjəˌgoʊ</strong> as an example, the transcription for eSpeak is:</p>\n\n<pre><code>$ espeak -v en --ipa \"[[,oUkeIh'i@j@g,oU]]\"\nˌəʊkeɪhˈiəjəɡˌəʊ\n</code></pre>\n\n<p>for British English, and:</p>\n\n<pre><code>$ espeak -v en-US --ipa \"[[,oUkeIh'i:rj@g,oU]]\"\nˌoʊkeɪhˈiːrjəɡˌoʊ\n</code></pre>\n\n<p>for American English. You can also get it to output the MBROLA phonemes, e.g.:</p>\n\n<pre><code>$ espeak -v mb-de5-en --pho \"[[,oUkeIh'i@j@g,oU]]\"\n@ w k E j h i: @ j @ g @ w\n</code></pre>\n\n<p>The MBROLA phonemes are actually listed one per line with duration and pitch contour information, but I have excluded that for brevity.</p>\n\n<p>Therefore, eSpeak would need to map <strong>ˌoʊkeɪˈhiːɹjəˌgoʊ</strong> to <code>,oUkeIh'i:rj@g,oU</code> for the en-US voice, <code>,oUkeIh'i@j@g,oU</code> for the en voice and <code>@ w k E j h i: @ j @ g @ w</code> for the MBROLA de5 voice.</p>\n", "Id":"3301", "ParentId":"3035", "LastEditorUserId":"185", "CreationDate":"2013-03-02T20:20:38.920", "OwnerUserId":"185", "PostTypeId":"2", "comments":[], "LastEditDate":"2013-03-04T18:19:44.137", "LastEditedUser":{"UpVotes":"2", "WebsiteUrl":"http://rhdunn.github.com", "Id":"185", "AccountId":"154673", "CreationDate":"2011-09-20T19:19:15.430", "AboutMe":"<p>I have interests in gui development (Qt4 and Gtk+3), accessibility, the web, semantic data, text-to-speech software, tools and libraries.</p>\n", "Age":"34", "Location":"United Kingdom", "DownVotes":"0", "Views":"3", "Reputation":"321", "DisplayName":"reece", "LastAccessDate":"2014-01-18T19:35:43.487"}, "LastActivityDate":"2013-03-04T18:19:44.137", "Score":"7", "CommentCount":"2", "OwnerUser":{"UpVotes":"2", "WebsiteUrl":"http://rhdunn.github.com", "Id":"185", "AccountId":"154673", "CreationDate":"2011-09-20T19:19:15.430", "AboutMe":"<p>I have interests in gui development (Qt4 and Gtk+3), accessibility, the web, semantic data, text-to-speech software, tools and libraries.</p>\n", "Age":"34", "Location":"United Kingdom", "DownVotes":"0", "Views":"3", "Reputation":"321", "DisplayName":"reece", "LastAccessDate":"2014-01-18T19:35:43.487"}}