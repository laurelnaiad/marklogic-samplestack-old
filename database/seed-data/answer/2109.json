{"OwnerUser":{"UpVotes":"1", "Location":"Ghent, Belgium", "DownVotes":"0", "Id":"663", "AccountId":"1013641", "Views":"0", "Reputation":"129", "CreationDate":"2012-01-11T08:02:27.857", "DisplayName":"Ruben Tavernier", "LastAccessDate":"2014-01-09T19:24:31.923", "AboutMe":"<p>MA in Linguistics, now studying Computer and Software Engineering. Interested in Karate, Mathematics, Physics, Computer Science, Language, History, Literature, Go/Baduk.</p>\n"}, "comments":[], "Body":"<p>This is just my two cents: I'm a graduated linguist, and a computer engineering student, but not a historian.</p>\n\n<p>I think your friend was talking from a much more practically minded perspective than the other repliers in this question are thinking, as you seem to anticipate in stating:</p>\n\n<blockquote>\n  <p>Note that by \"computational technology\", I'm including pretty much all the concepts involved, from the kind of machinery and electronics involved up to the programming languages that one can utilize those machines with.</p>\n</blockquote>\n\n<p>Of course it is equally possible to use, say, Chinese as a basis for computational interaction, or hieroglyphics, or whatever (heck, even a system with one ideogram per possible utterance could work, at least on a theoretical level). But there is a massive gap between what is possible theoretically and what practically.</p>\n\n<p>Remember that in the early days of practical computing you only got what you could dearly pay for. Bits were not the commodity they have now become. There were only a very small number of bits available, both in main and in secondary memory (read: RAM and hard disk).</p>\n\n<p>That means the problem was one of simple engineering: it is very unpractical to try to use, say, Chinese ideograms or any (really) large writing system when you only have a very limited amount of \"bit space\" available. You cannot even represent enough ideograms to state a meaningful program if you lack sufficient bit space, if you have to use 0-and-1 sequences to encode any data point.</p>\n\n<p>As an exercise for the interested reader: calculate how many bits would be necessary to represent all building blocks of your writing system using the most efficient binary encoding: <a href=\"http://en.wikipedia.org/wiki/Huffman_coding\">Huffman Coding Wikipedia article</a>. Then realise that to have a workable system, you would need to be able to use a large enough \"word\" (that is, a computer engineering word, which is the length of the every little sequence in memory and processing -- think 32bit or 64 bit). Then find the year since when systems with such word lenghts were economically valid.</p>\n", "Id":"2109", "ParentId":"2090", "CreationDate":"2012-06-14T19:26:51.867", "Score":"5", "PostTypeId":"2", "OwnerUserId":"663", "LastActivityDate":"2012-06-14T19:26:51.867", "CommentCount":"6"}