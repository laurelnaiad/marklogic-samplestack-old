{"OwnerUser":{"UpVotes":"512", "WebsiteUrl":"http://weew-waaw.com", "Id":"1009", "AccountId":"947951", "CreationDate":"2012-05-15T02:50:16.013", "AboutMe":"<p><img src=\"http://i.stack.imgur.com/VxMqM.jpg\" alt=\"Weew-Waaw, Thai to English, Thai to Russian transcription\"><br>\nI'm working on a project of rule-based Thai transcription: (the link is temporarily unavailable)<br>\nTechnically, it is written in F# with a C# front-end.<br>\nI would be happy to get acquainted with those who are experienced with NLP (Natural Language Processing), just ping me in the SE chat. :-)</p>\n", "Age":"42", "Location":"Bangkok, Thailand", "DownVotes":"50", "Views":"36", "Reputation":"1656", "DisplayName":"bytebuster", "LastAccessDate":"2014-01-19T00:53:20.473"}, "comments":[], "Body":"<p>TL;DR:<br>\nYou have a <strong>corpus</strong> and <strong>training set</strong>.</p>\n\n<ol>\n<li>Build the <strong>weighted</strong> n-grams for each word in the corpus; [data1]</li>\n<li>Build n-grams for the words in the training set; It will be <strong>unweighted</strong> since each word would have low frequency; [data2]</li>\n<li>Apply word weights from [data1] to [data2];</li>\n<li>Compute sentence weights for [data2]; [data3]</li>\n<li>Sort [data3] descending;</li>\n</ol>\n\n<hr>\n\n<p><a href=\"http://en.wikipedia.org/wiki/N-gram\" rel=\"nofollow\">N-gram</a> is essentially a set of N items (words), which is associated with a Markov property. This property can be called \"weight\" or \"probability\".<br>\nSay, a corpus contains a sentence:</p>\n\n<blockquote>\n  <p>I ate a green apple</p>\n</blockquote>\n\n<p>The <em>bigrams</em> for it would be: \"I ate\", \"ate a\", \"a green\", \"green apple\".<br>\nNote there can be funnier bigrams like \"I ??? a\", \"ate ??? green\", \"a ??? apple\".<br>\nThe <em>trigrams</em> are: \"I ate a\", \"ate a green\", and \"a green apple\".</p>\n\n<p>You have actually started moving this way. Computing a word frequency is essentially an <em>unigram</em>, a corner case of n-grams where the order of Markov chain equals to zero. A single word along with its weight is an unigram by itself.</p>\n\n<p><strong>Step 1, analyzing the corpus</strong><br>\nSay, your corpus contains 30M phrases and 30 of them contain a word pair \"green apple\". The bigram would be like:  </p>\n\n<pre><code>\"green\", \"apple\", 0.000001\n</code></pre>\n\n<p>Where <code>0.000001</code> is <code>30 / 30,000,000</code></p>\n\n<p>The result data would be an array of such structures.</p>\n\n<p><strong>Step 2, analyzing the training set</strong><br>\nBuild the same data for the training set.<br>\nYour training set most likely does not contain a representative number of sentences. But this is not a problem since you already have weights from a significantly larger corpus. Hence,</p>\n\n<pre><code>\"green\", \"apple\", ?\n</code></pre>\n\n<p><strong>Step 3</strong><br>\nAssign weights taken at Step 1 to the N-grams of Step 2.</p>\n\n<p><strong>Step 4, computing the weight of the sentence of the training set</strong><br>\nIt is a bit tricky because you have to invent a good mechanism of computing the weight of entire sentence here. Start with a plain multiplication, but get ready if depending on the nature of your training set, you will have to do a more sophisticated weight computation, for example, using unigrams and/or trigrams as well.</p>\n\n<p>Make sure that you gracefully handle zero values, here's why. Imagine your training set contains a greeting phrase like:</p>\n\n<blockquote>\n  <p>Hello John, welcome to my city</p>\n</blockquote>\n\n<p>Although it should be a high-rated phrase, \"John welcome\" may be missing from the corpus hence leading to multiplication to zero. This is called \"zero-frequency problem\". See <a href=\"http://en.wikipedia.org/wiki/N-gram#Smoothing_techniques\" rel=\"nofollow\">this article</a> for more details.</p>\n\n<p><strong>Step 5, getting the final result</strong><br>\nSince you have the weights of training sentences, just sort it along with its translation.</p>\n\n<hr>\n\n<p><strong>Tools</strong><br>\nI don't think you need to invent your own tools for building N-grams. There are plenty of those, here's just a <a href=\"http://www.ling.ohio-state.edu/~bromberg/ngramcount/ngramcount.html\" rel=\"nofollow\">single link</a>. You may just use them and then load the results into a spreadsheet program and write a small code of final computing and sorting.</p>\n", "Id":"6408", "ParentId":"6319", "CreationDate":"2014-01-15T06:14:04.260", "Score":"1", "PostTypeId":"2", "OwnerUserId":"1009", "LastActivityDate":"2014-01-15T06:14:04.260", "CommentCount":"5"}