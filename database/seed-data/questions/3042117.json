{"id":"/questions//questions/3042117.json.json", "creationDate":"2010-06-15T02:10:13.840", "body":"\n\nI hope that this one is not going to be \"ask-and-answer\" question... here goes:\n(multi)collinearity refers to extremely high correlations between predictors in the regression model. How to cure them... well, sometimes you don't need to \"cure\" collinearity, since it doesn't affect regression model itself, but interpretation of an effect of individual predictors.\n\nOne way to spot collinearity is to put each predictor as a dependent variable, and other predictors as independent variables, determine R<sup>2</sup>, and if it's larger than .9 (or .95), we can consider predictor redundant. This is one \"method\"... what about other approaches? Some of them are time consuming, like excluding predictors from model and watching for b-coefficient changes - they should be noticeably different.\n\nOf course, we must always bear in mind the specific context/goal of the analysis... Sometimes, only remedy is to repeat a research, but right now, I'm interested in various ways of screening redundant predictors when (multi)collinearity occurs in a regression model.", "lastActivityDate":"2013-12-31T22:42:05.460", "title":"Screening (multi)collinearity in a regression model", "tags":[["r"], ["statistics"], ["regression"]], "docScore":0, "comments":[], "answers":[], "creationYearMonth":"201006", "itemTally":0, "owner":null}